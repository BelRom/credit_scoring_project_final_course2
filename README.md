# Credit Score Project


Структура проекта
Репозиторий организован по принципам чистой архитектуры, разделяя логику приложения, модель машинного обучения и инфраструктурный код. Основные директории и файлы проекта:
    • app/ – исходный код сервиса кредитного скоринга (веб-приложение API). Здесь находится код на Python (например, с использованием Flask или FastAPI) для обработки запросов и выдачи результатов.
    • models/ – файлы, связанные с моделью машинного обучения: обучающие скрипты, сохраненные модели (например, в формате Pickle) и данные для обучения/валидации.
    • config/ – конфигурационные файлы приложения (например, config.yaml или файлы окружения .env), включая параметры модели, настройки базы данных и сервисов.
    • infra/ – инфраструктурный код и скрипты развёртывания: Dockerfile, docker-compose.yml для локального запуска, скрипты миграций базы данных и (при необходимости) манифесты для Kubernetes.
    • tests/ – тесты (юнит-тесты и интеграционные тесты) для проверки корректности работы компонентов системы. Тесты позволяют автоматизировать проверку качества при изменениях в коде.
    • .gitlab-ci.yml – конфигурация CI/CD pipeline для автоматизированной сборки, тестирования и деплоя (подробнее см. раздел CI/CD).
Пример структуры файлов в репозитории:
credit-scoring-project/
├── app/
│   ├── api.py              # основное приложение API (запуск сервера)
│   ├── models.py           # описание ML-модели и функция расчета скоринга
│   └── ...                 # другие модули приложения
├── models/
│   ├── train_model.py      # скрипт обучения модели
│   ├── model.pkl           # сохраненная ML-модель
│   └── ...                 # данные или вспомогательные скрипты
├── config/
│   ├── config.yaml         # основные настройки приложения
│   └── .env.example        # пример файла окружения с переменными
├── infra/
│   ├── docker-compose.yml  # конфигурация для локального запуска всех сервисов
│   ├── Dockerfile          # образ для сервисов приложения
│   └── k8s/                # (если используется) манифесты Kubernetes
├── tests/
│   ├── test_api.py         # тесты для API
│   └── test_model.py       # тесты для модели
├── .gitlab-ci.yml          # CI/CD пайплайн
└── README.md



# Этап 1. Подготовка модели к промышленной эксплуатации

source .venv/bin/activate
pip install -r requirements.txt

Обучение модели NN
python -m src.models_nn.train_nn \
  --data data/processed/credit_default.csv \
  --target target \
  --epochs 30

Сначала экспорт:
python -m src.models_nn.export_nn_to_onnx
Проверка корректности:
python -m src.models_nn.validate_nn_onnx
Бенчмарк CPU:
python -m src.models_nn.benchmark_nn_cpu


### экспорт ONNX
python -m src.models_nn.export_nn_to_onnx

#### quantization
python -m src.models_nn.optimization.quantize_torch_dynamic
python -m src.models_nn.optimization.quantize_onnx_int8

### метрики
python -m src.models_nn.optimization.evaluate_nn_variants

### benchmark CPU/GPU
python -m src.models_nn.optimization.benchmark_nn_variants

### отчет
python -m src.models_nn.optimization.report_benchmarks

## Оптимизация модели и оценка производительности

### Метод оптимизации

С целью повышения эффективности инференса была применена **посттренировочная квантизация INT8** нейронной сети.

Были рассмотрены следующие варианты:

* **Dynamic INT8-квантизация в PyTorch** для полносвязных (`Linear`) слоёв (поддерживается только на CPU).
* **INT8-квантизация в ONNX Runtime**, ориентированная на высокопроизводительный инференс на CPU.

Исходная модель в формате FP32 использовалась в качестве базовой для сравнения.

---

### Оценка качества модели

Качество модели оценивалось на отложенной тестовой выборке с использованием стандартных метрик классификации: ROC-AUC, Precision, Recall и F1-score.

**Результат:**
В результате INT8-квантизации размер модели был уменьшен в 3–3.5 раза по сравнению с FP32-версией без ухудшения качества предсказаний, что снижает требования к памяти и повышает эффективность инференса на CPU.

---

### Бенчмарк инференса

Производительность инференса измерялась при размере батча 1024 на CPU и GPU.

#### Результаты на CPU

| Вариант модели       | Задержка (мс / батч) | Пропускная способность (samples/s) |
| -------------------- | -------------------: | ---------------------------------: |
| Torch FP32           |                0.353 |                              2.90M |
| Torch INT8 (dynamic) |                0.345 |                              2.97M |
| ONNX FP32            |                0.431 |                              2.38M |
| **ONNX INT8**        |            **0.155** |                          **6.59M** |

INT8-модель в формате ONNX обеспечила **ускорение инференса более чем в 2.3 раза** по сравнению с базовой моделью Torch FP32.

#### Результаты на GPU

| Вариант модели | Задержка (мс / батч) |
| -------------- | -------------------: |
| Torch FP32     |                0.070 |
| ONNX FP32      |                0.083 |
| ONNX INT8      |                0.565 |

Использование INT8-квантизации на GPU не привело к ускорению из-за накладных расходов на передачу данных и особенностей выполнения.

---

### Выводы

* Квантизация INT8 существенно повышает производительность инференса на CPU без потери качества.
* **ONNX Runtime с INT8-квантизацией на CPU** является наиболее эффективной конфигурацией.
* Использование GPU целесообразно только для моделей в формате FP32; применение INT8 на GPU в данной задаче неэффективно.

**Рекомендуемая конфигурация для продакшена:**
 **CPU-инференс с использованием ONNX Runtime и INT8-квантизации**.


Этап 2: Инфраструктура как код (Terraform, Yandex Cloud)

Инфраструктура развёрнута в Yandex Cloud с помощью модульной конфигурации на Terraform. Настроены компоненты: VPC, Kubernetes-кластер (CPU/GPU node группы), Object Storage (для remote state) и мониторинг. Реализованы сетевые политики и security groups для безопасного взаимодействия сервисов.

Этап 3: Контейнеризация и оркестрация

Приложение собрано в оптимизированные Docker-образы с разделением frontend/backend. Поддержка DVC встроена в образы. Для Kubernetes созданы манифесты с Deployment (rolling update), ConfigMap/Secret, Service и Ingress для доступа к API.

Этап 4: CI/CD

CI/CD настроен на GitHub Actions. Pipeline включает сборку, тестирование, сканирование безопасности и автоматический деплой в staging/production. Обновления проходят без простоя. Расширенные стратегии деплоя (Blue-Green, Canary) не применялись, но возможны при необходимости.

Этап 5: Мониторинг и Observability

Используются Prometheus и Grafana для метрик, настроены дашборды производительности модели и API. Логирование через Grafana Loki. Alertmanager присылает оповещения при сбоях. Подготовлен runbook для реагирования на инциденты.

Этап 6: Мониторинг дрифта

Evidently AI интегрирован для отслеживания data и concept drift. Анализируются отклонения признаков и метрик модели. Управление версиями моделей (Seldon, A/B тесты) не реализовано, но архитектура допускает их добавление в будущем.

Заключение

Проект охватывает полный MLOps-цикл: от модели до мониторинга. Система надёжно работает в продакшене. Этап автоматического переобучения (Airflow и масштабирование) не выполнен, но предусмотрены заделы для будущего расширения.

